{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment this for using at Google Colab\n",
    "# !pip -q install nltk\n",
    "# !pip -q install torch\n",
    "# !pip -q install gensim\n",
    "# !pip -q install wordcloud\n",
    "# !pip -q install torchvision\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# path = \"gdrive/My Drive/AML/Project/Dataset/\"\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import pickle\n",
    "import string\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import seaborn as sb\n",
    "from torch import nn\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/arpan_aml/Project/dataset/'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'dataset.pkl', 'rb') as fp:\n",
    "    data = pickle.load(fp)\n",
    "trainX = data['trainX']\n",
    "trainY = data['trainY']\n",
    "testX = data['testX']\n",
    "testY = data['testY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_string(x):\n",
    "    ans = ''\n",
    "    for i in x:\n",
    "        ans += (i+' ')\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfIdf Embedding\n",
    "\n",
    "We tried creating a tf-idf embedding by considering all the questions as a single corporus. But dimension(no of unique words) was 84334. So for each question it had hugely sparsed embedding vector and no system was able to handle this kind of large dimentional input preprocessing, so we couldn't even reduce the dimension using some kind of dimentionality reduction technique(e.g.-PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(trainX)):\n",
    "    data.append(get_string(trainX[i][0]))\n",
    "    data.append(get_string(trainX[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def qstn_vectorization_tfidf(dataX, dataY):\n",
    "    q_1 = []\n",
    "    q_2 = []\n",
    "    Y = []\n",
    "    for ind in tqdm_notebook(range(len(dataX))):\n",
    "        q_1.append(vectorizer.transform([get_string(dataX[i][0])]).toarray()[0])\n",
    "        q_2.append(vectorizer.transform([get_string(dataX[i][1])]).toarray()[0])\n",
    "        Y.append(trainY[i])\n",
    "    q_1 = np.array(q_1)\n",
    "    q_2 = np.array(q_2)\n",
    "    X = np.concatenate((q_1, q_2), axis = 1)\n",
    "    Y = np.array(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5b365bfa904d1abe726d7b8e80ebdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=384329), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainX, trainY = qstn_vectorization(trainX, trainY)\n",
    "testX, testY = qstn_vectorization(testX, testY)\n",
    "data = {}\n",
    "data['trainX'] = trainX\n",
    "data['trainY'] = trainY\n",
    "data['testX'] = testX\n",
    "data['testY'] = testY\n",
    "with open(path+'dataset_tfidf.pkl', 'wb') as fp:\n",
    "    pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Glove Embedding\n",
    "We are considering glove 50d data file, it will return an embedding of 50 dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939cf5fee2334e82a3ef36e403f8aac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. 400001  words loaded!\n"
     ]
    }
   ],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding='utf-8')\n",
    "    model = {}\n",
    "    for line in tqdm_notebook(f):\n",
    "        try:\n",
    "            splitLine = line.split()\n",
    "            word = splitLine[0]\n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            model[word] = embedding\n",
    "        except:\n",
    "            print(line)\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "glove_model = loadGloveModel(path+'glove.6B.50d.txt')\n",
    "\n",
    "\n",
    "\n",
    "maxlen = 0\n",
    "for i in range(len(trainX)):\n",
    "    maxlen = max(maxlen, max(len(trainX[i][0]), len(trainX[i][1])))\n",
    "    \n",
    "for i in range(len(testX)):\n",
    "    maxlen = max(maxlen, max(len(testX[i][0]), len(testX[i][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(tokens):\n",
    "    zeroes = [0]*50\n",
    "    vec = []\n",
    "    for token in tokens:\n",
    "        if token in glove_model:\n",
    "            vec += list(glove_model[token])\n",
    "        else:\n",
    "            vec += zeroes\n",
    "    for _ in range(maxlen-len(tokens)):\n",
    "        vec += zeroes\n",
    "    return vec\n",
    "\n",
    "def qstn_vectorization_glove(dataX, dataY):\n",
    "    q_1 = []\n",
    "    q_2 = []\n",
    "    Y = []\n",
    "    for ind in tqdm_notebook(range(len(dataX))):\n",
    "        q_1.append(get_sentence_embedding(dataX[ind][0]))\n",
    "        q_2.append(get_sentence_embedding(dataX[ind][1]))\n",
    "        Y.append(dataY[i])\n",
    "    q_1 = np.array(q_1)\n",
    "    q_2 = np.array(q_2)\n",
    "    X = np.concatenate((q_1, q_2), axis = 1)\n",
    "    Y = np.array(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf878144543e427fb92f9c252e140c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=384329), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {}\n",
    "trainX, trainY = qstn_vectorization_glove(trainX, trainY)\n",
    "testX, testY = qstn_vectorization_glove(testX, testY)\n",
    "data['trainX'] = trainX\n",
    "data['trainY'] = trainY\n",
    "data['testX'] = testX\n",
    "data['testY'] = testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'dataset_glove.pkl', 'wb') as fp:\n",
    "    pickle.dump(data, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
